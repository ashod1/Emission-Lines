{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eeae0c6-7960-45c3-bd66-2c98f705678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import random\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import KDTree\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "from os import listdir\n",
    "import desispec\n",
    "import desispec.io\n",
    "import speclite.filters\n",
    "import scipy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23ac883-75b3-4b7e-bb84-0589417adb87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## reading fluxes and equivalent widths\n",
    "\n",
    "run=2\n",
    "fluxes_bin=np.load(\"/global/homes/a/ashodkh/results/fluxes_bin_\"+str(run)+\".txt.npz\")[\"arr_0\"]\n",
    "zs=np.load(\"/global/homes/a/ashodkh/results/zs_s_\"+str(run)+\".txt.npz\")[\"arr_0\"]\n",
    "target_lines=np.load(\"/global/homes/a/ashodkh/results/target_lines_s_\"+str(run)+\".txt.npz\")[\"arr_0\"]\n",
    "target_ids=np.load(\"/global/homes/a/ashodkh/results/target_ids_s_\"+str(run)+\".txt.npz\")[\"arr_0\"]\n",
    "\n",
    "n=25*10**3\n",
    "lines=[\"OII_DOUBLET_EW\",\"HGAMMA_EW\",\"HBETA_EW\",\"OIII_4959_EW\",\"OIII_5007_EW\",\"NII_6548_EW\"\\\n",
    "       ,\"HALPHA_EW\",\"NII_6584_EW\",\"SII_6716_EW\",\"SII_6731_EW\"]\n",
    "EWs=np.zeros([n,len(lines)])\n",
    "cosmo = FlatLambdaCDM(H0=70., Om0=0.3)\n",
    "magnitudes=np.zeros([n,fluxes_bin.shape[1]])\n",
    "\n",
    "## selecting only positive fluxes and saving them to use in other codes\n",
    "select_fluxes=fluxes_bin[:,0]>0\n",
    "for i in range(1,fluxes_bin.shape[1]):\n",
    "    select_fluxes=select_fluxes*(fluxes_bin[:,i]>0)\n",
    "np.savez_compressed(\"/global/homes/a/ashodkh/results/select_positive_fluxes_\"+str(run)+\".txt\", select_fluxes)\n",
    "fluxes_bin=fluxes_bin[select_fluxes,:]\n",
    "target_lines=target_lines[select_fluxes]\n",
    "target_ids=target_ids[select_fluxes]\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    #Dl=10**6*cosmo.luminosity_distance(zs[i]).value\n",
    "    magnitudes[i,:]=-2.5*np.log10(fluxes_bin[i,:])#-5*np.log10(Dl/10)\n",
    "    for j in range(len(lines)):\n",
    "        EWs[i,j]=target_lines[i][j]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff218104-9692-45e6-88b4-903d8e84f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLR_inverse_distance(x1,x1_train,y1_train,nn):\n",
    "    '''\n",
    "    Local linear regression with inverse distance weight and nn number of nearest neighbors.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    x1: Matrix of features in the shape (number of data points, number of features). Outcome will be evaluated at these points.\n",
    "    \n",
    "    x1_train: Matrix of features used for training in the shape of (number of data points, number of features).\n",
    "    \n",
    "    y1_train: Matrix of outcomes used for training in the shape of (number of data points, number of outcomes).\n",
    "    \n",
    "    nn: Number of nearest neighbors to include for each point.\n",
    "    \n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    \n",
    "    EW_fit: Matrix of outcomes predicte in the same shape as y1_train.\n",
    "    \n",
    "    zeros: indices corresponding to points in x1 that have nearest neighbor at zero distance.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    nl=nn\n",
    "    tree=KDTree(x1_train[:,:])\n",
    "    dist, ind=tree.query(x1[:,:],k=nl)\n",
    "\n",
    "    # removing points on top of each other\n",
    "    zeros=np.where(dist==0)[0]\n",
    "    print(zeros)\n",
    "    dist=np.delete(dist,obj=zeros,axis=0)\n",
    "    ind=np.delete(ind,obj=zeros,axis=0)\n",
    "    x1=np.delete(x1,obj=zeros,axis=0)\n",
    "\n",
    "    n_valid=x1.shape[0]\n",
    "\n",
    "    theta=np.zeros([n_valid,x1.shape[1],1])\n",
    "    W=np.zeros([n_valid,nl,nl])\n",
    "    X=np.zeros([n_valid,nl,x1.shape[1]])\n",
    "    Y=np.zeros([n_valid,nl,1])\n",
    "    for j in range(nl):\n",
    "        W[:,j,j]=1/dist[:,j]\n",
    "        X[:,j,:]=x1_train[ind[:,j],:]\n",
    "        Y[:,j,0]=y1_train[ind[:,j]]\n",
    "    a1=np.zeros([n_valid,x1.shape[1],1])\n",
    "    a2=np.zeros([n_valid,x1.shape[1],x1.shape[1]])\n",
    "    EW_fit=np.zeros(n_valid)\n",
    "    for ii in range(n_valid):\n",
    "        a1[ii,:,:]=np.matmul(X[ii,:,:].transpose(),np.matmul(W[ii,:,:],Y[ii,:,:]))\n",
    "        a2[ii,:,:]=np.matmul(X[ii,:,:].transpose(),np.matmul(W[ii,:,:],X[ii,:,:]))\n",
    "        theta[ii,:,:]=np.matmul(np.linalg.inv(a2[ii,:,:]),a1[ii,:,:])\n",
    "        EW_fit[ii]=np.matmul(theta[ii,:,:].transpose(),x_valid[ii,:])\n",
    "        \n",
    "    return EW_fit,zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d5a3e3-bfe5-40b5-87f0-2451fc6b61dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "HALPHA_EW\n",
      "[0.8762985642717703, 0.8833131348661016, 0.8729118949299033, 0.8684810956289755, 0.8625146854583498, 0.8621055963688955, 0.8618396118143381, 0.871363555226169, 0.8739042908646867, 0.8817880217900836]\n",
      "0.8714520451219274\n",
      "[0.19580962345535483, 0.19735639981285627, 0.1993750298475247, 0.20175721186299353, 0.20148762271764326, 0.19443759553261505, 0.2007357327399384, 0.20099639537097472, 0.19661135898719315, 0.19979457074705226]\n",
      "0.19883615410741462\n",
      "[0.19580962345535483, 0.19735639981285627, 0.1993750298475247, 0.20175721186299353, 0.20148762271764326, 0.19443759553261505, 0.2007357327399384, 0.20099639537097472, 0.19661135898719315, 0.19979457074705226]\n",
      "0.19883615410741462\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ones=np.ones([n,1])\n",
    "x=np.zeros([n,magnitudes.shape[1]-1])\n",
    "for i in range(n):\n",
    "    for j in range(magnitudes.shape[1]-1):\n",
    "        x[i,j]=magnitudes[i,j]-magnitudes[i,j+1]\n",
    "x=np.concatenate((ones,x),axis=1)\n",
    "\n",
    "# av_x=np.zeros(x.shape[1]-1)\n",
    "# std_x=np.zeros(x.shape[1]-1)\n",
    "# for i in range(1,x.shape[1]):\n",
    "#     av_x[i-1]=np.average(x[:,i])\n",
    "#     std_x[i-1]=np.std(x[:,i])\n",
    "#     x[:,i]=(x[:,i]-av_x[i-1])/std_x[i-1]\n",
    "    \n",
    "for l in range(1):\n",
    "    l=6\n",
    "    EW=np.log10(EWs[:,l])    \n",
    "    N_cv=10\n",
    "    x_split=np.split(x,N_cv)\n",
    "    EW_split=np.split(EW,N_cv)\n",
    "\n",
    "    EW_fit_all=[]\n",
    "    EW_obs_all=[]\n",
    "\n",
    "    spearman_all=[]\n",
    "    rms_all=[]\n",
    "    nmad_all=[]\n",
    "    nmad2_all=[]\n",
    "    for i in range(N_cv):\n",
    "        ## assigning the training and validation sets\n",
    "        x_valid=x_split[i]\n",
    "        EW_valid=EW_split[i]\n",
    "\n",
    "        x_to_combine=[]\n",
    "        EW_to_combine=[]\n",
    "        for j in range(N_cv):\n",
    "            if j!=i:\n",
    "                x_to_combine.append(x_split[j])\n",
    "                EW_to_combine.append(EW_split[j])\n",
    "        x_train=np.concatenate(tuple(x_to_combine),axis=0)\n",
    "        EW_train=np.concatenate(tuple(EW_to_combine),axis=0)\n",
    "        \n",
    "        # predicting EWs using LLR\n",
    "        EW_fit,zeros=LLR_inverse_distance(x_valid,x_train,EW_train,100)\n",
    "        \n",
    "        # removing points that are on top of each other from y_valid and its ivar\n",
    "        EW_valid=np.delete(EW_valid,obj=zeros,axis=0)\n",
    "\n",
    "        \n",
    "        # calculating spearman coefficient and nmad for fit. nmad2 has the error in it.\n",
    "        nmad=np.abs(EW_fit-EW_valid)\n",
    "        nmad2=np.abs(EW_fit-EW_valid)\n",
    "\n",
    "        EW_fit_all.append(EW_fit)\n",
    "        EW_obs_all.append(EW_valid)\n",
    "\n",
    "        spearman_all.append(stats.spearmanr(EW_fit,EW_valid)[0])\n",
    "        rms_all.append(np.sqrt(mean_squared_error(EW_fit,EW_valid)))\n",
    "        nmad_all.append(1.48*np.median(nmad))\n",
    "        nmad2_all.append(1.48*np.median(nmad2))\n",
    "\n",
    "    print(lines[l])\n",
    "    print(spearman_all)\n",
    "    print(np.average(spearman_all))\n",
    "    # print(rms_all)\n",
    "    # print(np.average(rms_all))\n",
    "    print(nmad_all)\n",
    "    print(np.average(nmad_all))\n",
    "    print(nmad2_all)\n",
    "    print(np.average(nmad2_all))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #np.savez_compressed(\"/global/homes/a/ashodkh/results/logEW_fit_bins_selection\"+str(run)+\"_line\"+str(lines[l])+\".txt\",EW_fit_all)\n",
    "    #np.savez_compressed(\"/global/homes/a/ashodkh/results/logEW_obs_bins_selection\"+str(run)+\"_line\"+str(lines[l])+\".txt\",EW_obs_all)\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESI master",
   "language": "python",
   "name": "desi-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
