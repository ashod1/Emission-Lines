{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad63ce4a-bc9e-4295-83ee-5e1c703e947c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/a/ashodkh/.conda/envs/myenv/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.table import Table,join\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import random\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import KDTree\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from desitarget.targetmask import desi_mask, bgs_mask, mws_mask\n",
    "from LLR import LLR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import xgboost as xgb\n",
    "from LLR import LLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "762dd977-d8d5-494d-9d5c-b894e5f0c6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: MergeConflictWarning: Cannot merge meta key 'EXTNAME' types <class 'str'> and <class 'str'>, choosing EXTNAME='FASTSPEC' [astropy.utils.metadata]\n",
      "WARNING: MergeConflictWarning: Cannot merge meta key 'EXTNAME' types <class 'str'> and <class 'str'>, choosing EXTNAME='FASTPHOT' [astropy.utils.metadata]\n",
      "/tmp/ipykernel_46097/2565242007.py:22: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  data.add_column(1/(1/data[\"OII_3726_EW_IVAR\"]+1/data[\"OII_3729_EW_IVAR\"]),name='OII_DOUBLET_EW_IVAR')\n"
     ]
    }
   ],
   "source": [
    "## DATA ##\n",
    "## I'm combining fastphot,fastspect, and ztile to make sure I use the same data everywhere ##\n",
    "\n",
    "zall_path=\"/project/projectdirs/desi/spectro/redux/everest/zcatalog/ztile-main-bright-cumulative.fits\"\n",
    "data1=Table.read(zall_path,hdu=1)\n",
    "needed1=[\"TARGETID\",\"BGS_TARGET\",\"SPECTYPE\",\"DELTACHI2\",\"Z\",\"ZWARN\"]\n",
    "\n",
    "fastspec_path = \"/project/projectdirs/desi/spectro/fastspecfit/everest/catalogs/fastspec-everest-main-bright.fits\"\n",
    "data2=Table.read(fastspec_path,hdu=1)\n",
    "needed2=[\"TARGETID\",\"OII_3726_EW\",\"OII_3729_EW\",\"HGAMMA_EW\",\"HBETA_EW\",\"OIII_4959_EW\",\"OIII_5007_EW\",\"NII_6548_EW\",\"HALPHA_EW\",\"NII_6584_EW\",\"SII_6716_EW\",\"SII_6731_EW\",\\\n",
    "        \"OII_3726_EW_IVAR\",\"OII_3729_EW_IVAR\",\"HGAMMA_EW_IVAR\",\"HBETA_EW_IVAR\",\"OIII_4959_EW_IVAR\",\"OIII_5007_EW_IVAR\",\"NII_6548_EW_IVAR\",\"HALPHA_EW_IVAR\",\"NII_6584_EW_IVAR\",\"SII_6716_EW_IVAR\",\"SII_6731_EW_IVAR\"]\n",
    "\n",
    "file_path = \"/project/projectdirs/desi/spectro/fastspecfit/everest/catalogs/fastphot-everest-main-bright.fits\"\n",
    "data3=Table.read(file_path,hdu=1)\n",
    "needed3=[\"TARGETID\",\"ABSMAG_SDSS_U\",\"ABSMAG_SDSS_G\",\"ABSMAG_SDSS_R\",\"ABSMAG_SDSS_I\",\"ABSMAG_SDSS_Z\"]\n",
    "\n",
    "data4=join(data1[needed1],data2[needed2],keys=\"TARGETID\")\n",
    "data=join(data4,data3[needed3],keys=\"TARGETID\")\n",
    "\n",
    "## Adding the sum of OII doublets to use them as a single line\n",
    "data.add_column(data[\"OII_3726_EW\"]+data[\"OII_3729_EW\"],name='OII_DOUBLET_EW')\n",
    "data.add_column(1/(1/data[\"OII_3726_EW_IVAR\"]+1/data[\"OII_3729_EW_IVAR\"]),name='OII_DOUBLET_EW_IVAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e6e3dfc-6f65-4a2b-a3e3-e6d1f19a1556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450905\n",
      "88\n",
      "84\n",
      "100\n",
      "128\n",
      "102\n",
      "116\n",
      "109\n",
      "108\n",
      "121\n",
      "117\n",
      "HALPHA_EW\n",
      "[0.8286405322456807, 0.8437531784409005, 0.83503858154145, 0.8388695934183125, 0.8319204025108918, 0.8305854259888491, 0.8306374543715562, 0.8343620521136281, 0.8480888446476605, 0.854672413037204]\n",
      "av spearman = 0.8376568478316134\n",
      "[0.23274305880069732, 0.2445535969734192, 0.23675756335258483, 0.23262191772460938, 0.23531399309635162, 0.2277190452814102, 0.22133491277694703, 0.22613953590393066, 0.23803266525268554, 0.22177841305732726]\n",
      "av nmad = 0.23169947022199633\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Selecting data and doing LLR to predict lines ##\n",
    "lines=[\"OII_DOUBLET_EW\",\"HGAMMA_EW\",\"HBETA_EW\",\"OIII_4959_EW\",\"OIII_5007_EW\",\"NII_6548_EW\",\"HALPHA_EW\"\\\n",
    "       ,\"NII_6584_EW\",\"SII_6716_EW\",\"SII_6731_EW\"]\n",
    "\n",
    "magnitude_names=[\"ABSMAG_SDSS_U\",\"ABSMAG_SDSS_G\",\"ABSMAG_SDSS_R\",\"ABSMAG_SDSS_I\",\"ABSMAG_SDSS_Z\"]\n",
    " \n",
    "N=len(data[\"TARGETID\"])\n",
    "snr_cut=1 # signal to noise ratio cut\n",
    "n=30*10**3 # initial selection size. Should be smaller later after selecting flux>0 from raw data to make sure same data is used.\n",
    "\n",
    "# calculating snr for all lines and setting the snr cut as select_snr\n",
    "snr_all=np.zeros([N,len(lines)])\n",
    "snr_all[:,0]=data[lines[0]]*np.sqrt(data[lines[0]+\"_IVAR\"])\n",
    "\n",
    "cut_Halpha=True\n",
    "cut_all=False\n",
    "for i in range(1,len(lines)):\n",
    "    snr_all[:,i]=data[lines[i]]*np.sqrt(data[lines[i]+\"_IVAR\"])\n",
    "    if cut_all:\n",
    "        select_snr=select_snr*(snr_all[:,i]>snr_cut)\n",
    "if cut_Halpha:\n",
    "    select_snr=snr_all[:,6]>snr_cut\n",
    "\n",
    "# calculating minimum redshift to have de-redshifted wavelengths be in the interval 3400,7000 A\n",
    "w1=3400\n",
    "w_min=3600\n",
    "z_min=w_min/w1-1\n",
    "\n",
    "#parameters\n",
    "ll=6\n",
    "N=16\n",
    "run_flux=1\n",
    "run_out=2\n",
    "loga=True     # if true then predicts log(EW)\n",
    "m=3           # model index. 0 is LLR, 1 is RandomForest, 2 is GradientBoosting from sklearn, 3 is XGboost, 4 is neural network\n",
    "\n",
    "# getting flux cut from everest target selection to make sure same data is used\n",
    "select_fluxes=np.load(\"/global/cscratch1/sd/ashodkh/results/select_positive_fluxes_selection\"+str(run_flux)+\"_\"+str(lines[ll])+\"_bins\"+str(N)+\".txt.npz\")[\"arr_0\"]\n",
    "for l in range(1):\n",
    "    l=ll\n",
    "    # initial target selection that doesn't include flux cut\n",
    "    select=((data[\"BGS_TARGET\"] & bgs_mask.mask(\"BGS_BRIGHT\"))>0)*(data[\"SPECTYPE\"]==\"GALAXY\")*(data[\"DELTACHI2\"]>=25)\\\n",
    "        *(data[\"Z\"]>z_min)*(data[\"Z\"]<0.3)*(data[\"ZWARN\"]==0)*select_snr*(snr_all[:,l]>0)\n",
    "    target_ids=data[\"TARGETID\"][select]\n",
    "    print(len(np.where(select==True)[0]))\n",
    "    target_pos=np.where(select==True)[0][:n] \n",
    "    \n",
    "    # flux cut after initial target selection and taking first n data\n",
    "    n=25*10**3\n",
    "    target_pos=target_pos[select_fluxes][:n]\n",
    "\n",
    "    # assigning features as colors and standardizing them. I also add ones to include the y-intercept as part of the parameter matrix if m==0 (LLR).\n",
    "    magnitudes_s=data[magnitude_names][target_pos]  \n",
    "    magnitudes=np.zeros([n,len(magnitude_names)])\n",
    "    for j in range(len(magnitude_names)):\n",
    "        magnitudes[:,j]=magnitudes_s[magnitude_names[j]][:n]\n",
    "\n",
    "    ones=np.ones([n,1])\n",
    "    x=np.zeros([n,len(magnitude_names)-1])\n",
    "    for i in range(n):\n",
    "        for j in range(len(magnitude_names)-1):\n",
    "            x[i,j]=magnitudes[i,j]-magnitudes[i,j+1]\n",
    "    av_x=np.zeros(x.shape[1])\n",
    "    std_x=np.zeros(x.shape[1])\n",
    "    for i in range(x.shape[1]):\n",
    "        av_x[i]=np.average(x[:,i])\n",
    "        std_x[i]=np.std(x[:,i])\n",
    "        x[:,i]=(x[:,i]-av_x[i])/std_x[i]\n",
    "    \n",
    "    if m==0:\n",
    "        x=np.concatenate((ones,x),axis=1)\n",
    "    \n",
    "    # assigning outcomes as EW (equivalent width) and getting their inverse variance\n",
    "    if loga:\n",
    "        EW=np.log10(data[lines[l]][target_pos])\n",
    "    else:\n",
    "        EW=data[lines[l]][target_pos]\n",
    "    ivar=data[lines[l]+\"_IVAR\"][target_pos]\n",
    "    \n",
    "    ## doing cross-validation by splitting data into N_cv intervals. I store all the outcomes in EW_fit_all, ivar_all, etc...\n",
    "    N_cv=10\n",
    "    x_split=np.split(x,N_cv)\n",
    "    EW_split=np.split(EW,N_cv)\n",
    "    ivar_split=np.split(ivar,N_cv)\n",
    "    \n",
    "    EW_fit_all=[]\n",
    "    EW_obs_all=[]\n",
    "    ivar_all=[]\n",
    "    \n",
    "    spearman_all=[]\n",
    "    nmad_all=[]\n",
    "    for i in range(N_cv):\n",
    "        ## assigning the training and validation sets\n",
    "        x_valid=x_split[i]\n",
    "        EW_valid=EW_split[i]\n",
    "        ivar_valid=ivar_split[i]\n",
    "        x_to_combine=[]\n",
    "        EW_to_combine=[]\n",
    "        for j in range(N_cv):\n",
    "            if j!=i:\n",
    "                x_to_combine.append(x_split[j])\n",
    "                EW_to_combine.append(EW_split[j])\n",
    "        x_train=np.concatenate(tuple(x_to_combine),axis=0)\n",
    "        EW_train=np.concatenate(tuple(EW_to_combine),axis=0)\n",
    "        \n",
    "        # predicting EWs using LLR\n",
    "        if m==0:\n",
    "            EW_fit,zeros=LLR.LLR(x_valid, x_train, EW_train, 100, 'inverse_distance')\n",
    "        if m==1:\n",
    "            model=RandomForestRegressor(n_estimators=200)\n",
    "            model.fit(x_train, EW_train)\n",
    "            EW_fit=model.predict(x_valid)\n",
    "        if m==2:\n",
    "            model=GradientBoostingRegressor(n_estimators=100)\n",
    "            model.fit(x_train, EW_train)\n",
    "            EW_fit=model.predict(x_valid)\n",
    "        if m==3:\n",
    "            model=xgb.XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "            model.fit(x_train, EW_train, early_stopping_rounds=5, eval_set=[(x_valid,EW_valid)], verbose=False)\n",
    "            EW_fit=model.predict(x_valid)\n",
    "            print(model.best_ntree_limit)\n",
    "        if m==4:\n",
    "            model=keras.Sequential([\n",
    "                layers.Dense(units=10, activation='sigmoid', input_shape=[x.shape[1]]),\n",
    "                layers.Dense(units=5, activation='sigmoid'),\n",
    "                layers.Dense(units=3, activation='sigmoid'),\n",
    "                layers.Dense(units=1),\n",
    "            ])\n",
    "            model.compile(optimizer='Adam', loss='mse')\n",
    "            model.fit(x_train, EW_train,batch_size=5)\n",
    "            EW_fit=model.predict(x_valid)\n",
    "        \n",
    "        # removing points that are on top of each other from y_valid and its ivar\n",
    "        EW_valid=np.delete(EW_valid,obj=zeros,axis=0)\n",
    "        ivar_valid=np.delete(ivar_valid,obj=zeros,axis=0)\n",
    "        \n",
    "        # calculating spearman coefficient and nmad for fit.\n",
    "        nmad=np.abs(EW_fit-EW_valid)\n",
    "\n",
    "        EW_fit_all.append(EW_fit)\n",
    "        EW_obs_all.append(EW_valid)\n",
    "        ivar_all.append(ivar_valid)\n",
    "        \n",
    "        spearman_all.append(stats.spearmanr(EW_fit,EW_valid)[0])\n",
    "        nmad_all.append(1.48*np.median(nmad))\n",
    "\n",
    "    print(lines[l])\n",
    "    print(spearman_all)\n",
    "    print(\"av spearman = \"+str(np.average(spearman_all)))\n",
    "    print(nmad_all)\n",
    "    print(\"av nmad = \"+str(np.average(nmad_all)))\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    if loga:\n",
    "        np.savez_compressed(\"/global/cscratch1/sd/ashodkh/results/logEW_fit_classical_selection\"+str(run_out)+\"_line\"+str(lines[l])+\"_bins\"+str(N)+\"_model\"+str(m)+\".txt\",EW_fit_all)\n",
    "        np.savez_compressed(\"/global/cscratch1/sd/ashodkh/results/logEW_obs_classical_selection\"+str(run_out)+\"_line\"+str(lines[l])+\"_bins\"+str(N)+\"_model\"+str(m)+\".txt\",EW_obs_all)\n",
    "        np.savez_compressed(\"/global/cscratch1/sd/ashodkh/results/logEW_ivar_classical_selection\"+str(run_out)+\"_line\"+str(lines[l])+\"_bins\"+str(N)+\"_model\"+str(m)+\".txt\",ivar_all)\n",
    "    else:\n",
    "        np.savez_compressed(\"/global/cscratch1/sd/ashodkh/results/EW_fit_classical_selection\"+str(run_out)+\"_line\"+str(lines[l])+\"_bins\"+str(N)+\"_model\"+str(m)+\".txt\",EW_fit_all)\n",
    "        np.savez_compressed(\"/global/cscratch1/sd/ashodkh/results/EW_obs_classical_selection\"+str(run_out)+\"_line\"+str(lines[l])+\"_bins\"+str(N)+\"_model\"+str(m)+\".txt\",EW_obs_all)\n",
    "        np.savez_compressed(\"/global/cscratch1/sd/ashodkh/results/EW_ivar_classical_selection\"+str(run_out)+\"_line\"+str(lines[l])+\"_bins\"+str(N)+\"_model\"+str(m)+\".txt\",ivar_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b470d3-1fe3-4d6c-8ebe-3bfb55086d68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
