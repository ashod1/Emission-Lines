{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad63ce4a-bc9e-4295-83ee-5e1c703e947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.table import Table,join\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import random\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import KDTree\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from desitarget.targetmask import desi_mask, bgs_mask, mws_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "762dd977-d8d5-494d-9d5c-b894e5f0c6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: MergeConflictWarning: Cannot merge meta key 'EXTNAME' types <class 'str'> and <class 'str'>, choosing EXTNAME='FASTSPEC' [astropy.utils.metadata]\n",
      "WARNING: MergeConflictWarning: Cannot merge meta key 'EXTNAME' types <class 'str'> and <class 'str'>, choosing EXTNAME='FASTPHOT' [astropy.utils.metadata]\n",
      "/tmp/ipykernel_33239/4143185914.py:22: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  data.add_column(1/(data[\"OII_3726_EW_IVAR\"]+data[\"OII_3729_EW_IVAR\"]),name='OII_DOUBLET_EW_IVAR')\n"
     ]
    }
   ],
   "source": [
    "## DATA ##\n",
    "## I'm combining fastphot,fastspect, and ztile to make sure I use the same data everywhere ##\n",
    "\n",
    "zall_path=\"/project/projectdirs/desi/spectro/redux/everest/zcatalog/ztile-main-bright-cumulative.fits\"\n",
    "data1=Table.read(zall_path,hdu=1)\n",
    "needed1=[\"TARGETID\",\"BGS_TARGET\",\"SPECTYPE\",\"DELTACHI2\",\"Z\",\"ZWARN\"]\n",
    "\n",
    "fastspec_path = \"/project/projectdirs/desi/spectro/fastspecfit/everest/catalogs/fastspec-everest-main-bright.fits\"\n",
    "data2=Table.read(fastspec_path,hdu=1)\n",
    "needed2=[\"TARGETID\",\"OII_3726_EW\",\"OII_3729_EW\",\"HGAMMA_EW\",\"HBETA_EW\",\"OIII_4959_EW\",\"OIII_5007_EW\",\"NII_6548_EW\",\"HALPHA_EW\",\"NII_6584_EW\",\"SII_6716_EW\",\"SII_6731_EW\",\\\n",
    "        \"OII_3726_EW_IVAR\",\"OII_3729_EW_IVAR\",\"HGAMMA_EW_IVAR\",\"HBETA_EW_IVAR\",\"OIII_4959_EW_IVAR\",\"OIII_5007_EW_IVAR\",\"NII_6548_EW_IVAR\",\"HALPHA_EW_IVAR\",\"NII_6584_EW_IVAR\",\"SII_6716_EW_IVAR\",\"SII_6731_EW_IVAR\"]\n",
    "\n",
    "file_path = \"/project/projectdirs/desi/spectro/fastspecfit/everest/catalogs/fastphot-everest-main-bright.fits\"\n",
    "data3=Table.read(file_path,hdu=1)\n",
    "needed3=[\"TARGETID\",\"ABSMAG_SDSS_U\",\"ABSMAG_SDSS_G\",\"ABSMAG_SDSS_R\",\"ABSMAG_SDSS_I\",\"ABSMAG_SDSS_Z\"]\n",
    "\n",
    "data4=join(data1[needed1],data2[needed2],keys=\"TARGETID\")\n",
    "data=join(data4,data3[needed3],keys=\"TARGETID\")\n",
    "\n",
    "## Adding the sum of OII doublets to use them as a single line\n",
    "data.add_column(data[\"OII_3726_EW\"]+data[\"OII_3729_EW\"],name='OII_DOUBLET_EW')\n",
    "data.add_column(1/(data[\"OII_3726_EW_IVAR\"]+data[\"OII_3729_EW_IVAR\"]),name='OII_DOUBLET_EW_IVAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f116c7-cec6-4c14-b590-a341d457499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLR_inverse_distance(x1,x1_train,y1_train,nn):\n",
    "    '''\n",
    "    Local linear regression with inverse distance weight and nn number of nearest neighbors.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    x1: Matrix of features in the shape (number of data points, number of features). Outcome will be evaluated at these points.\n",
    "    \n",
    "    x1_train: Matrix of features used for training in the shape of (number of data points, number of features).\n",
    "    \n",
    "    y1_train: Matrix of outcomes used for training in the shape of (number of data points, number of outcomes).\n",
    "    \n",
    "    nn: Number of nearest neighbors to include for each point.\n",
    "    \n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    \n",
    "    EW_fit: Matrix of outcomes predicte in the same shape as y1_train.\n",
    "    \n",
    "    zeros: indices corresponding to points in x1 that have nearest neighbor at zero distance.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    nl=nn\n",
    "    tree=KDTree(x1_train[:,:])\n",
    "    dist, ind=tree.query(x1[:,:],k=nl)\n",
    "\n",
    "    # removing points on top of each other\n",
    "    zeros=np.where(dist==0)[0]\n",
    "    print(zeros)\n",
    "    dist=np.delete(dist,obj=zeros,axis=0)\n",
    "    ind=np.delete(ind,obj=zeros,axis=0)\n",
    "    x1=np.delete(x1,obj=zeros,axis=0)\n",
    "\n",
    "    n_valid=x1.shape[0]\n",
    "\n",
    "    theta=np.zeros([n_valid,x1.shape[1],1])\n",
    "    W=np.zeros([n_valid,nl,nl])\n",
    "    X=np.zeros([n_valid,nl,x1.shape[1]])\n",
    "    Y=np.zeros([n_valid,nl,1])\n",
    "    for j in range(nl):\n",
    "        W[:,j,j]=1/dist[:,j]\n",
    "        X[:,j,:]=x1_train[ind[:,j],:]\n",
    "        Y[:,j,0]=y1_train[ind[:,j]]\n",
    "    a1=np.zeros([n_valid,x1.shape[1],1])\n",
    "    a2=np.zeros([n_valid,x1.shape[1],x1.shape[1]])\n",
    "    EW_fit=np.zeros(n_valid)\n",
    "    for ii in range(n_valid):\n",
    "        a1[ii,:,:]=np.matmul(X[ii,:,:].transpose(),np.matmul(W[ii,:,:],Y[ii,:,:]))\n",
    "        a2[ii,:,:]=np.matmul(X[ii,:,:].transpose(),np.matmul(W[ii,:,:],X[ii,:,:]))\n",
    "        theta[ii,:,:]=np.matmul(np.linalg.inv(a2[ii,:,:]),a1[ii,:,:])\n",
    "        EW_fit[ii]=np.matmul(theta[ii,:,:].transpose(),x_valid[ii,:])\n",
    "        \n",
    "    return EW_fit,zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e6e3dfc-6f65-4a2b-a3e3-e6d1f19a1556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33239/3971532732.py:13: RuntimeWarning: invalid value encountered in multiply\n",
      "  snr_all[:,0]=data[lines[0]]*np.sqrt(data[lines[0]+\"_IVAR\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450905\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "HALPHA_EW\n",
      "[0.8430676459308234, 0.8533380708700914, 0.8506669272747084, 0.8502175596508097, 0.8411498582319775, 0.8447751198680193, 0.8375019292331092, 0.8431041428326629, 0.8598901625424261, 0.8681872931246588]\n",
      "0.8491898709559287\n",
      "[0.21913059181935426, 0.22943219019237532, 0.21771232796662662, 0.21791980236789607, 0.22535438173943828, 0.2191766745266001, 0.2117075073402828, 0.21807189194944449, 0.2196206799078542, 0.21246111322130576]\n",
      "0.2190587161031178\n",
      "[0.18353955126947796, 0.18806804097176347, 0.1795397731021607, 0.19006909580457879, 0.18901691453981287, 0.18478190765825073, 0.18190642388472575, 0.1821542870207031, 0.17983681445474778, 0.1820495402295734]\n",
      "0.18409623489357946\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Selecting data and doing LLR to predict lines ##\n",
    "lines=[\"OII_DOUBLET_EW\",\"HGAMMA_EW\",\"HBETA_EW\",\"OIII_4959_EW\",\"OIII_5007_EW\",\"NII_6548_EW\",\"HALPHA_EW\"\\\n",
    "       ,\"NII_6584_EW\",\"SII_6716_EW\",\"SII_6731_EW\"]\n",
    "\n",
    "magnitude_names=[\"ABSMAG_SDSS_U\",\"ABSMAG_SDSS_G\",\"ABSMAG_SDSS_R\",\"ABSMAG_SDSS_I\",\"ABSMAG_SDSS_Z\"]\n",
    " \n",
    "N=len(data[\"TARGETID\"])\n",
    "snr_cut=1 # signal to noise ratio cut\n",
    "n=30*10**3 # initial selection size. Should be smaller later after selecting flux>0 from raw data to make sure same data is used.\n",
    "\n",
    "# calculating snr for all lines and setting the snr cut as select_snr\n",
    "snr_all=np.zeros([N,len(lines)])\n",
    "snr_all[:,0]=data[lines[0]]*np.sqrt(data[lines[0]+\"_IVAR\"])\n",
    "\n",
    "cut_Halpha=True\n",
    "cut_all=False\n",
    "for i in range(1,len(lines)):\n",
    "    snr_all[:,i]=data[lines[i]]*np.sqrt(data[lines[i]+\"_IVAR\"])\n",
    "    if cut_all:\n",
    "        select_snr=select_snr*(snr_all[:,i]>snr_cut)\n",
    "if cut_Halpha:\n",
    "    select_snr=snr_all[:,6]>snr_cut\n",
    "\n",
    "# calculating minimum redshift to have de-redshifted wavelengths be in the interval 3400,7000 A\n",
    "w1=3400\n",
    "w_min=3600\n",
    "z_min=w_min/w1-1\n",
    "\n",
    "# getting flux cut from everest target selection to make sure same data is used\n",
    "run=2\n",
    "select_fluxes=np.load(\"/global/homes/a/ashodkh/results/select_positive_fluxes_\"+str(run)+\".txt.npz\")[\"arr_0\"]\n",
    "for l in range(1):\n",
    "    l=6\n",
    "    # initial target selection that doesn't include flux cut\n",
    "    select=((data[\"BGS_TARGET\"] & bgs_mask.mask(\"BGS_BRIGHT\"))>0)*(data[\"SPECTYPE\"]==\"GALAXY\")*(data[\"DELTACHI2\"]>=25)\\\n",
    "        *(data[\"Z\"]>z_min)*(data[\"Z\"]<0.3)*(data[\"ZWARN\"]==0)*select_snr\n",
    "    target_ids=data[\"TARGETID\"][select][:n]\n",
    "    print(len(np.where(select==True)[0]))\n",
    "    target_pos=np.where(select==True)[0][:n] \n",
    "    # flux cut after initial target selection and taking first n data\n",
    "    n=25*10**3\n",
    "    target_pos=target_pos[select_fluxes][:n]\n",
    "    target_ids=target_ids[select_fluxes]\n",
    "    np.savetxt(\"/global/homes/a/ashodkh/results/target_ids_test1.txt\", target_ids[:n])\n",
    "    # assigning features as colors and standardizing them. I also add ones to include the y-intercept as part of the parameter matrix\n",
    "    magnitudes_s=data[magnitude_names][target_pos]  \n",
    "    magnitudes=np.zeros([n,len(magnitude_names)])\n",
    "    for j in range(len(magnitude_names)):\n",
    "        magnitudes[:,j]=magnitudes_s[magnitude_names[j]][:n]\n",
    "\n",
    "    ones=np.ones([n,1])\n",
    "    x=np.zeros([n,len(magnitude_names)-1])\n",
    "    for i in range(n):\n",
    "        for j in range(len(magnitude_names)-1):\n",
    "            x[i,j]=magnitudes[i,j]-magnitudes[i,j+1]\n",
    "    x=np.concatenate((ones,x),axis=1)\n",
    "\n",
    "    av_x=np.zeros(x.shape[1]-1)\n",
    "    std_x=np.zeros(x.shape[1]-1)\n",
    "    for i in range(1,x.shape[1]):\n",
    "        av_x[i-1]=np.average(x[:,i])\n",
    "        std_x[i-1]=np.std(x[:,i])\n",
    "        x[:,i]=(x[:,i]-av_x[i-1])/std_x[i-1]\n",
    "    \n",
    "    # assigning outcomes as EW (equivalent width) and getting their inverse variance\n",
    "    EW=np.log10(data[lines[l]][target_pos])\n",
    "    ivar=data[lines[l]+\"_IVAR\"][target_pos]\n",
    "    \n",
    "    ## doing cross-validation by splitting data into N_cv intervals. I store all the outcomes in EW_fit_all, ivar_all, etc...\n",
    "    N_cv=10\n",
    "    x_split=np.split(x,N_cv)\n",
    "    EW_split=np.split(EW,N_cv)\n",
    "    ivar_split=np.split(ivar,N_cv)\n",
    "    EW_fit_all=[]\n",
    "    EW_obs_all=[]\n",
    "    ivar_all=[]\n",
    "    spearman_all=[]\n",
    "    rms_all=[]\n",
    "    nmad_all=[]\n",
    "    nmad2_all=[]\n",
    "    for i in range(N_cv):\n",
    "        ## assigning the training and validation sets\n",
    "        x_valid=x_split[i]\n",
    "        EW_valid=EW_split[i]\n",
    "        ivar_valid=ivar_split[i]\n",
    "        x_to_combine=[]\n",
    "        EW_to_combine=[]\n",
    "        for j in range(N_cv):\n",
    "            if j!=i:\n",
    "                x_to_combine.append(x_split[j])\n",
    "                EW_to_combine.append(EW_split[j])\n",
    "        x_train=np.concatenate(tuple(x_to_combine),axis=0)\n",
    "        EW_train=np.concatenate(tuple(EW_to_combine),axis=0)\n",
    "        \n",
    "        # predicting EWs using LLR\n",
    "        EW_fit,zeros=LLR_inverse_distance(x_valid,x_train,EW_train,100)\n",
    "        \n",
    "        # removing points that are on top of each other from y_valid and its ivar\n",
    "        EW_valid=np.delete(EW_valid,obj=zeros,axis=0)\n",
    "        ivar_valid=np.delete(ivar_valid,obj=zeros,axis=0)\n",
    "        \n",
    "        # calculating spearman coefficient and nmad for fit. nmad2 has the error in it.\n",
    "        nmad=np.abs(EW_fit-EW_valid)\n",
    "        nmad2=np.abs(EW_fit-EW_valid)*np.sqrt(ivar_valid)*EW_valid\n",
    "\n",
    "        EW_fit_all.append(EW_fit)\n",
    "        EW_obs_all.append(EW_valid)\n",
    "        ivar_all.append(ivar_valid)\n",
    "        spearman_all.append(stats.spearmanr(EW_fit,EW_valid)[0])\n",
    "        rms_all.append(np.sqrt(mean_squared_error(EW_fit,EW_valid)))\n",
    "        nmad_all.append(1.48*np.median(nmad))\n",
    "        nmad2_all.append(1.48*np.median(nmad2))\n",
    "\n",
    "    print(lines[l])\n",
    "    print(spearman_all)\n",
    "    print(np.average(spearman_all))\n",
    "    # print(rms_all)\n",
    "    # print(np.average(rms_all))\n",
    "    print(nmad_all)\n",
    "    print(np.average(nmad_all))\n",
    "    print(nmad2_all)\n",
    "    print(np.average(nmad2_all))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    ## saving data\n",
    "    \n",
    "    \n",
    "    # #with open(\"/global/homes/a/ashodkh/results/logEW_fit_all_nl300_exp\"+str(alpha)+\".txt\",'w') as f:\n",
    "    # with open(\"/global/homes/a/ashodkh/results/logEW_fit_snr\"+str(snr_cut)+\"_all_nl\"+str(nl)+\"_distance\"+lines[l]+\".txt\",'w') as f:    \n",
    "    #     for i in range(N_cv):\n",
    "    #         for j in range(len(EW_fit_all[i])-1):\n",
    "    #             f.write(str(np.round(EW_fit_all[i][j],decimals=5))+',')\n",
    "    #         f.write(str(np.round(EW_fit_all[i][-1],decimals=5)))\n",
    "    #         f.write('\\n')\n",
    "    # #with open(\"/global/homes/a/ashodkh/results/logEW_obs_all_nl300_exp\"+str(alpha)+\".txt\",'w') as f:\n",
    "    # with open(\"/global/homes/a/ashodkh/results/logEW_obs_snr\"+str(snr_cut)+\"_all_nl\"+str(nl)+\"_distance\"+lines[l]+\".txt\",'w') as f: \n",
    "    #     for i in range(N_cv):\n",
    "    #         for j in range(len(EW_obs_all[i])-1):\n",
    "    #             f.write(str(np.round(EW_obs_all[i][j],decimals=5))+',')\n",
    "    #         f.write(str(np.round(EW_obs_all[i][-1],decimals=5)))\n",
    "    #         f.write('\\n')\n",
    "    # with open(\"/global/homes/a/ashodkh/results/logEW_snr\"+str(snr_cut)+\"_ivar\"+lines[l]+\".txt\",'w') as f: \n",
    "    #     for i in range(N_cv):\n",
    "    #         for j in range(len(ivar_all[i])-1):\n",
    "    #             f.write(str(np.round(ivar_all[i][j],decimals=5))+',')\n",
    "    #         f.write(str(np.round(ivar_all[i][-1],decimals=5)))\n",
    "    #         f.write('\\n')\n",
    "\n",
    "    \n",
    "    # np.savez_compressed(\"/global/homes/a/ashodkh/results/logEW_fit_classical_selection\"+str(run)+\"_line\"+str(lines[l])+\".txt\",EW_fit_all)\n",
    "    # np.savez_compressed(\"/global/homes/a/ashodkh/results/logEW_obs_classical_selection\"+str(run)+\"_line\"+str(lines[l])+\".txt\",EW_obs_all)\n",
    "    # np.savez_compressed(\"/global/homes/a/ashodkh/results/logEW_ivar_classical_selection\"+str(run)+\"_line\"+str(lines[l])+\".txt\",ivar_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b19039bd-c111-47ec-8202-a785707fbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(EW_obs_all[0],EW_fit_all[0],'*',alpha=0.1)\n",
    "# plt.plot(np.arange(0,4,0.1),np.arange(0,4,0.1))\n",
    "# # plt.xlim((0,4))\n",
    "# plt.ylim((0,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45296e49-8741-4ef1-97e2-ad0f555b4258",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESI master",
   "language": "python",
   "name": "desi-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
